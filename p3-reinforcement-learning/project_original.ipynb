{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## MO444 Project 3 - Reinforcement Learning\n",
    "\n",
    "* Rodrigo Ara√∫jo Marinho Franco - RA: 233569\n",
    "* Felipe Marinho Tavares - RA: 265680\n",
    "\n",
    "### Contributions:\n",
    "* Rodrigo\n",
    " * ...\n",
    "* Felipe\n",
    " * ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PART I - Evolutionary Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PART II - Reinforcement Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"search/\")\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Medium Environment - Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (25, 26, 1)\n",
      "Episode:  1 Average Score: -479.00 Winrate: 0.00%\n",
      "Episode:  100 Average Score: -264.04 Winrate: 0.00%\n",
      "Episode:  200 Average Score: -108.85 Winrate: 0.00%\n",
      "Episode:  300 Average Score: 164.28 Winrate: 0.00%\n",
      "Episode:  400 Average Score: 337.18 Winrate: 0.00%\n",
      "Episode:  500 Average Score: 507.72 Winrate: 0.00%\n",
      "Episode:  600 Average Score: 492.46 Winrate: 2.00%\n",
      "Episode:  700 Average Score: 337.47 Winrate: 0.00%\n",
      "Episode:  800 Average Score: 481.44 Winrate: 1.00%\n",
      "Episode:  900 Average Score: 518.68 Winrate: 3.00%\n",
      "Episode:  1000 Average Score: 505.27 Winrate: 2.00%\n",
      "Episode:  1100 Average Score: 534.57 Winrate: 1.00%\n",
      "Episode:  1200 Average Score: 627.33 Winrate: 3.00%\n",
      "Episode:  1300 Average Score: 651.42 Winrate: 6.00%\n",
      "Episode:  1400 Average Score: 659.96 Winrate: 2.00%\n",
      "Episode:  1500 Average Score: 741.61 Winrate: 7.00%\n",
      "Episode:  1600 Average Score: 791.55 Winrate: 9.00%\n",
      "Episode:  1700 Average Score: 828.10 Winrate: 13.00%\n",
      "Episode:  1800 Average Score: 981.03 Winrate: 19.00%\n",
      "Episode:  1900 Average Score: 1009.23 Winrate: 17.00%\n",
      "Episode:  2000 Average Score: 1148.89 Winrate: 28.00%\n",
      "Episode:  2100 Average Score: 1029.12 Winrate: 19.00%\n",
      "Episode:  2200 Average Score: 966.48 Winrate: 21.00%\n",
      "Episode:  2300 Average Score: 1135.68 Winrate: 28.00%\n",
      "Episode:  2400 Average Score: 1260.37 Winrate: 38.00%\n",
      "Episode:  2500 Average Score: 1111.98 Winrate: 29.00%\n",
      "Episode:  2600 Average Score: 1252.09 Winrate: 37.00%\n",
      "Episode:  2700 Average Score: 1162.96 Winrate: 36.00%\n",
      "Episode:  2800 Average Score: 1179.24 Winrate: 34.00%\n",
      "Episode:  2900 Average Score: 1151.58 Winrate: 34.00%\n",
      "Episode:  3000 Average Score: 967.16 Winrate: 21.00%\n",
      "Episode:  3100 Average Score: 922.27 Winrate: 23.00%\n",
      "Episode:  3200 Average Score: 1129.51 Winrate: 30.00%\n",
      "Episode:  3300 Average Score: 1171.41 Winrate: 35.00%\n",
      "Episode:  3400 Average Score: 1078.20 Winrate: 28.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-98c4b2587204>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[1;32mwhile\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdone\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m         \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0meps\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[0meps\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0meps_end\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meps_decay\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0meps\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mT:\\projects\\mo444-machine-learning\\p3-reinforcement-learning\\search\\environment.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, eps)\u001B[0m\n\u001B[0;32m    262\u001B[0m                     \u001B[0mlegal\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetLegalPacmanActions\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    263\u001B[0m                     \u001B[0mlegal\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mremove\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDirections\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSTOP\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 264\u001B[1;33m                     \u001B[0maction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetAction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate_pacman\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlegal\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meps\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    265\u001B[0m                     \u001B[0maction_pacman\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_action_as_number\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    266\u001B[0m                 \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mT:\\projects\\mo444-machine-learning\\p3-reinforcement-learning\\search\\dqnAgent.py\u001B[0m in \u001B[0;36mgetAction\u001B[1;34m(self, state, legal_actions, eps)\u001B[0m\n\u001B[0;32m     93\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mqnetwork_local\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meval\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     94\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 95\u001B[1;33m             \u001B[0maction_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mqnetwork_local\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     96\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mqnetwork_local\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     97\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\mo443\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mT:\\projects\\mo444-machine-learning\\p3-reinforcement-learning\\search\\model.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m    168\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    169\u001B[0m         \u001B[1;34m\"\"\"Build a network that maps state -> action values.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 170\u001B[1;33m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    171\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    172\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\mo443\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\mo443\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    137\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    138\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 139\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    140\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    141\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\mo443\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mT:\\projects\\mo444-machine-learning\\p3-reinforcement-learning\\search\\model.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     26\u001B[0m         \u001B[0mbias\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mbias\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepsilon_bias\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormal_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m             \u001B[0mbias\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbias\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msigma_bias\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepsilon_bias\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msigma_weight\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepsilon_weight\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"buffer_size\": int(5e5),  # replay buffer size\n",
    "    \"batch_size\": 32,        # minibatch size\n",
    "    \"gamma\": 0.99,            # discount factor\n",
    "    \"tau\": 1e-3,              # for soft update of target parameters\n",
    "    \"lr\": 1e-3,               # learning rate\n",
    "    \"update_every\": 10         # how often to update the network\n",
    "}\n",
    "\n",
    "# layouts = smallClassic, mediumClassic, originalClassic\n",
    "env = environment.Environment(params, layout=\"originalClassic\", use_features=True)\n",
    "\n",
    "eps_start = 1.0   # Starting exploration rate\n",
    "#eps_end = 0.05    # Minimun exploration rate (0.05 = 5%)\n",
    "eps_start = 0.0\n",
    "eps_end = 0.0\n",
    "eps_decay=0.999\n",
    "n_episodes = 10000\n",
    "print_every = 100 # episodes\n",
    "patience = 15      # max print_every without improvement tolarated\n",
    "checkpoints_without_improvement = 0\n",
    "\n",
    "eps = eps_start\n",
    "best_winrate = -1\n",
    "for i_episode in range(1, n_episodes + 1):\n",
    "    env.reset()\n",
    "\n",
    "    while not env.done():\n",
    "        env.step(eps)\n",
    "\n",
    "    eps = max(eps_end, eps_decay*eps)\n",
    "\n",
    "    env.scores.append(env.game.state.getScore())\n",
    "    env.scores_window.append(env.game.state.getScore())\n",
    "\n",
    "    env.wins.append(env.game.state.isWin())\n",
    "    env.wins_window.append(env.game.state.isWin())\n",
    "\n",
    "    average_score = sum(env.scores_window)/float(len(env.scores_window))\n",
    "    winrate = sum(env.wins_window)/float(len(env.wins_window)) * 100.0\n",
    "    env.average_scores.append(average_score)\n",
    "    env.average_wins.append(winrate)\n",
    "\n",
    "    if i_episode == 1 or i_episode % print_every == 0:\n",
    "        print(\"Episode: \", i_episode, end=\" \")\n",
    "        print('Average Score: {:.2f}'.format(average_score), end=\" \")\n",
    "        print('Winrate: {:.2f}%'.format(winrate))\n",
    "\n",
    "        if winrate > best_winrate:\n",
    "            checkpoints_without_improvement = 0\n",
    "            best_score = average_score\n",
    "            torch.save(env.pacman.qnetwork_local.state_dict(), 'models/checkpoint_original.pth')\n",
    "        else:\n",
    "            checkpoints_without_improvement += 1\n",
    "            if checkpoints_without_improvement == patience:\n",
    "                print(\"Stopping training due to lack of improvement\")\n",
    "                break\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Scores during training\")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "line1, = plt.plot(env.scores)\n",
    "line2, = plt.plot(env.average_scores)\n",
    "plt.legend([line1, line2], [\"Score\", \"Average Score\"])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Wins during training\")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Number of Wins')\n",
    "line1, = plt.plot(env.average_wins)\n",
    "plt.legend([line1], [\"Average Wins\"])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-ef6924e5",
   "language": "python",
   "display_name": "PyCharm (p3-reinforcement-learning)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}